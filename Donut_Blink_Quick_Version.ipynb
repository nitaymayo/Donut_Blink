{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlmOXDz+OcCoLkyxNwtwOE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitaymayo/Donut_Blink/blob/master/Donut_Blink_Quick_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Donut Blink - Short Version\n",
        "\n",
        "this notebook will demonstrate just the Donut_Blink model preformance as a pretraind model<br>\n",
        "*for the full code of the model please visit the full notebook - <a href=\"https://github.com/nitaymayo/Donut_Blink/blob/master/Donut_Blink.ipynb\">here</a>"
      ],
      "metadata": {
        "id": "P1XQEV_H4f0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "_IuxV4rQ5YZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X-7TzZwQ4XAL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.patches as patches\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib.patches import Circle\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eyes clipping algorithms"
      ],
      "metadata": {
        "id": "Ez7uT8S05b58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/nitaymayo/Donut_Blink/raw/master/shape_predictor_68_face_landmarks.dat\n",
        "\n",
        "# Face detector\n",
        "hog_face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# Eye detector\n",
        "dlib_facelandmark = dlib.shape_predictor(\"/content/shape_predictor_68_face_landmarks.dat\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqHflf1HBrok",
        "outputId": "28b5e062-0364-49d1-a0cb-5d98e09409b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 07:49:39--  https://github.com/nitaymayo/Donut_Blink/raw/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nitaymayo/Donut_Blink/master/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2023-04-03 07:49:40--  https://raw.githubusercontent.com/nitaymayo/Donut_Blink/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.1’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   175MB/s    in 0.5s    \n",
            "\n",
            "2023-04-03 07:49:41 (175 MB/s) - ‘shape_predictor_68_face_landmarks.dat.1’ saved [99693937/99693937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clipping the eyes from the pictures\n",
        "y_factor = 18\n",
        "x_factor = 15\n",
        "\n",
        "def eye_box(img, left_point, right_point):\n",
        "  \"\"\"\n",
        "    Function to get the coordinates for the box around the eye\n",
        "    Args:\n",
        "    img: img of the full face\n",
        "    left_point: the left corner coordinates of the eye in the img\n",
        "    right point: the right corner coordinates of the eye in the img\n",
        "\n",
        "    Returns:\n",
        "    tuple as so:\n",
        "    in index 0: tuple -> (the lower left corner coordinates of th box, the upper right corner coordinates of the box)\n",
        "    in index 1: img of the eye cropped as the box dims\n",
        "  \"\"\"\n",
        "  corner_l_l = (left_point.x-x_factor, left_point.y+y_factor)\n",
        "\n",
        "  corner_u_r = (right_point.x+x_factor, right_point.y-y_factor)\n",
        "\n",
        "  X_axis_coordinates = (corner_l_l[0], corner_u_r[0])\n",
        "  Y_axis_coordinates = (corner_l_l[1], corner_u_r[1])\n",
        "\n",
        "  return ((corner_l_l, corner_u_r), img[tf.math.reduce_min(Y_axis_coordinates):tf.math.reduce_max(Y_axis_coordinates), tf.math.reduce_min(X_axis_coordinates):tf.math.reduce_max(X_axis_coordinates)])"
      ],
      "metadata": {
        "id": "KiMR3mh85oNL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "def get_eyes_imgs(img, img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "    Function to get all the eyes in a given image\n",
        "    The function gets the eye coordinate using the \n",
        "    dlib facelandmark detector and crops the eye img \n",
        "    with the above function 'eye_box()'\n",
        "\n",
        "    Args:\n",
        "    img: image in array form\n",
        "    img_size: eye img size to be returned \n",
        "\n",
        "    Returns:\n",
        "    Array with dictioneries as so: {'img':contains cropped eye img, 'coordinates': tuple with the (lower left, upper right) points of the box}\n",
        "  \"\"\"\n",
        "  img = np.asarray(img)\n",
        "  eyes = []\n",
        "  #Getting eyes position\n",
        "\n",
        "  faces = hog_face_detector(img)\n",
        "\n",
        "  if not faces:\n",
        "    return False\n",
        "\n",
        "  for face in faces:\n",
        "    face_landmarks = dlib_facelandmark(img, face)\n",
        "\n",
        "    left_coordinate, left_eye = eye_box(img, face_landmarks.part(36), face_landmarks.part(39))\n",
        "\n",
        "    right_coordinate, right_eye = eye_box(img, face_landmarks.part(42), face_landmarks.part(45))\n",
        "\n",
        "    left_eye = cv2.resize(left_eye, img_size)\n",
        "    right_eye = cv2.resize(right_eye, img_size)\n",
        "\n",
        "    eyes.append({\"img\": left_eye,\n",
        "                 \"coordinates\": left_coordinate})\n",
        "    eyes.append({\"img\": right_eye,\n",
        "                \"coordinates\": right_coordinate})\n",
        "\n",
        "  return eyes"
      ],
      "metadata": {
        "id": "fCa3dTMW5qOB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the pretrained model"
      ],
      "metadata": {
        "id": "9t2-43dM88Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/nitaymayo/Donut_Blink/raw/master/eye_state_detector_fine_tuned.h5\n",
        "\n",
        "final_model = tf.keras.models.load_model(\"eye_state_detector_fine_tuned.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tstTErYK8-32",
        "outputId": "9964afda-d329-479f-d238-9eca704938d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 07:49:44--  https://github.com/nitaymayo/Donut_Blink/raw/master/eye_state_detector_fine_tuned.h5\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nitaymayo/Donut_Blink/master/eye_state_detector_fine_tuned.h5 [following]\n",
            "--2023-04-03 07:49:45--  https://raw.githubusercontent.com/nitaymayo/Donut_Blink/master/eye_state_detector_fine_tuned.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24390504 (23M) [application/octet-stream]\n",
            "Saving to: ‘eye_state_detector_fine_tuned.h5.1’\n",
            "\n",
            "eye_state_detector_ 100%[===================>]  23.26M  99.5MB/s    in 0.2s    \n",
            "\n",
            "2023-04-03 07:49:45 (99.5 MB/s) - ‘eye_state_detector_fine_tuned.h5.1’ saved [24390504/24390504]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The final function\n",
        "this function gathers the cropping function and the model prediction to give the final result: \n",
        "1. eye box coordinates\n",
        "2. eye state"
      ],
      "metadata": {
        "id": "kZmUbt0-9EOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eyes_bbox_state(img, model):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "    img: to predict on\n",
        "    model: to make the predictions\n",
        "\n",
        "    Returns:\n",
        "    array with dictioneries {'coordinates': eye box coordinates, \n",
        "                             'state': 0 for closed eye and 1 for open one}\n",
        "  \"\"\"\n",
        "  # Seperate the img to seperate eye imgs\n",
        "  eyes = get_eyes_imgs(img)\n",
        "  if not eyes:\n",
        "      return False\n",
        "  res = []\n",
        "  eyes_imgs = np.asarray([eye[\"img\"] for eye in eyes])\n",
        "  preds = tf.squeeze(np.round(model.predict(eyes_imgs, verbose=0)))\n",
        "  for i, eye in enumerate(eyes):\n",
        "    # Getting eye state\n",
        "    eye_state = {\"coordinates\": eye[\"coordinates\"]}\n",
        "    state = np.round(model.predict(tf.expand_dims(eye[\"img\"], axis=0), verbose=0)[0])\n",
        "\n",
        "    eye_state[\"state\"] = preds[i]\n",
        "\n",
        "    res.append(eye_state)    \n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "fkuXAAFD9Pka"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Java-script configuration for using the web cam to show the models predictions"
      ],
      "metadata": {
        "id": "3uk4ZuaZBEbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "91wnSzLWBNSJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "RE09EtyOBPRu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn on the webcam and see the model predictions"
      ],
      "metadata": {
        "id": "MFAP8OhbBQ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # get face region coordinates\n",
        "    eyes = get_eyes_bbox_state(img, final_model)\n",
        "    if eyes: \n",
        "      # get face bounding box for overlay\n",
        "      for eye in eyes:\n",
        "        color = (255,0,0) if eye[\"state\"] == 0 else (0,255,0)\n",
        "\n",
        "        bbox_array = cv2.rectangle(bbox_array,eye[\"coordinates\"][0],eye[\"coordinates\"][1],color,2)\n",
        "\n",
        "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "      # convert overlay of bbox into bytes\n",
        "      bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "      # update bbox so next frame gets new overlay\n",
        "      bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "NVJols12BWcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}